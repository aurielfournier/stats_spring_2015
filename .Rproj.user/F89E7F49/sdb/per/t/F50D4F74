{
    "contents" : "---\ntitle: 'GEOS 4863: Modeling Point Patterns using *spatstat*'\nauthor: \"Jackson Cothren and Panagiotis Giannakis\"\ndate: \"January 29, 2015\"\noutput:\n  html_document:\n    fig_caption: yes\n    keep_md: yes\n    number_sections: yes\n    theme: united\n    toc: yes\n  pdf_document:\n    toc: yes\n  word_document: default\n---\n\n# Working with a point pattern\nFirst load the spatstat library. We will be using R Version 3.1.2 (2014-10-31) and spatstat version 1.40-0. We'll also load the maptools and sp libraries because they have useful objects and utility functions for working with spatial data. Maptools depends on sp so the latter will load automatically.\n\n```{r}\nlibrary(spatstat)\nlibrary(maptools)\n```\n\nParts I and II of Baddeley's CSIRO workshop provide details on working with the various objects in spatstat. For now, we allow that to stand on it's own and jump to various description statistics and techniques to describe a pattern. The next section addresses intensity estimates, the section after that takes a look at interaction effects, and the final section shows how to use your esimates of intensity variations (first order effects) and interactions (second order effects) to develop and test regression models. \n\n#Understanding and modeling the intensity of a point pattern (first order effects).\n\n##Quadrat counting\nOne of the parameters of interesting in modeling spatial point processes is the intensity of the pattern. The intensity is simply the density of points in the region of interest (rememeber that we need the points and the region in which they can appear in order to have a meaningful point pattern). The intensity might be homogenous - or the same - across the entire region. Or the intensity might be inhomogenous - or variable - across the region. This section provides methods for assessing whether the intensity homongenous or inhomogenous and if it is inhomonogenous, what function can be written to describe how it varies across the region.  \n\nFirst, load some data that is supplied with spatstat\n```{r}\ndata(swedishpines)  # type the command ?swedishpines for more information\nsummary(swedishpines) # note the units (decimeters)\n\nswedishpines.meter <- rescale(swedishpines) # lets rescale it to meters, you can work with either\nsummary(swedishpines.meter)\nplot(swedishpines.meter)\n```\n\nThe summary command shows you the details of the ppp dataset including the intensity, $\\lambda$ (assuming its homogenous, just the number of points divided by the region area), the window or regions size and shape, and the unit of measure. We can extract any of those from the ppp object using the $ operator.\n\n```{r}\nlambda <- summary(swedishpines.meter)$intensity\n```\n\nIf, however, we observe that the intensity changes across the region, we investiage the possibility of an inhomogenous intensity measure caused by some \"outside\" influence. This could be an unknown influence that you just end up modeling with a funciton (like a NW trend) or a known, or suspected, covariate (such as a soil type, distance to water, slope, land cover type, etc.). You will often see the term \"intensity measure\" when $\\lambda$ is inhomongenous and written as $\\lambda(s)$ where $s$ is the location within the window (or region).\n\nOne of the oldest methods avialable to assess $\\lambda(s)$ is quadrat counting. The method is described in the lecture notes and involves tesselating the region into a number of subregions, or quadrats. The quadrats are traditional square and of equal area, but this isn't required. As we saw in class hexagonal quadrats have some geometric properties that make them appealing. Further, the quadrats can also be irregular in shape and size conforming to some particular aspect of the environment in the point pattern emerges (ie. defined by covariates).\n\nLet's start simply and use the traditional quadrat method in *spatstat*...\n\n```{r}\ndata(bei) # a point pattern giving the locations of 3605 trees in a tropical rain forrest. (It also contains covariates which well use later, try summary(bei))\nquadratcount(bei,nx=4,ny=2)  #tesselates the regions into four columns and two rows of subregions and countes the number of points in each subregion\nQ <- quadratcount(bei,nx=6,ny=3) # re-tesselate into 18 subregions and save as a quadratcount object\nplot(bei, cex = 0.5, pch = \"+\") # plot the point pattern\nplot(Q, add=TRUE, cex =2) # and plot the quadrats on top\n```\n\n## Kernel Density Estimation (KDE)\nClearly, the intensity in not homogenous across the region (the quadrats would have similar point counts if it was). That means we need to investiage what an intensity measure, $\\lambda(s)$, might look like. One way to do this is using a kernel density estimator (KDE). These create \"moving averages\" of intensity across the region based on some averaging technique. The \"kernel\" determines the technique. As we saw in the lecture notes the kernel can vary in size (larger sizes mean averaging over larger areas) and shape (circular, box, or gaussian).\n\nWe can use the density.ppp command to create an *im* object showing how the intensity or density of points varies across the region. In this case, we use a guassian kernal with a $\\sigma$ of 70 units (since the unit of bei dataset is meters the units of $\\sigma$ are also meters).\n\n```{r}\nden <- density.ppp(bei,sigma=70, kernel='gaussian') #a fairly large kernel\nplot(den)\nplot(bei, add = TRUE, cex=0.1)\n\nden <- density.ppp(bei,sigma=35, kernel='gaussian') #a smaller kernel showing more localized variations in density\nplot(den)\nplot(bei, add = TRUE, cex=0.1)\n```\n\nBy default *plot* displays the result as a colormapped images. However, you also have the option of displaying 3D perspective and contour plots. You also can export the image to a GIS compatible file. \n\n```{r}\npersp(den)\npersp(den, theta = 20, phi = 70) #change the viewpoint\ncontour(den, axes=FALSE)\n```\n\nAdaptive density is another inhomogenous density visualization method. It uses a random fraction, f, \nof the points to compute a direchlet tesselation (creating voronoi polygons) of the data. Within in each voronoi polygon it computes $\\lambda$. This is done \"nrep\" times and averages the result at each *im* cell (each cell is 1 unit square) to get a better estimate.\n\n```{r}\naden <- adaptive.density(bei,f=0.01,nrep=10)\nplot(aden, main = \"Adaptive Intensity\")\nplot(bei, add=TRUE, cex=0.1)\n```\n\nAgain, quadrats don't have to be hexagonal or rectangular and many meaningful tesselation of the region is based on covariates.\n\n```{r}\nZ <- bei.extra$grad  # extract the gradient map covering the region of the point pattern (it's an im object)\n\nb <- quantile(Z, probs=(0:4)/4) # compute the quartiles of gradient values\nZcut <- cut(Z, breaks=b,labels=c('low','mlow','mhigh','high'))   # name the quartiles sensibly\nV <- tess(image = Zcut) # create a tess object based on the cuts\nplot(V) # this should make it clear what we've done. \nplot(bei,add=T,pch='+')\nqb <- quadratcount(bei,tess=V) # use the tess object to create the quadrats\n```\n\nIf the regions aren't equal area, then we would need to compute the intensity of the point pattern in each region by dividing the count by the region area. *spatstat* does this for us in the *quadrat.test* function.\n\nSo the quadrat method works for covariates that are - or can be made - categorical (as we did in the preceding code block. We can also assume the intensity of the point process is a function of the \"continuous\" valued covariate, Z. So at any spatial location $s$, $\\lambda(s) = \\rho(Z(s))$ where $Z$ is the spatial covariate (like the gradient above).  \n\nThe function *rhohat* estimates $\\rho(Z(s))$ using a variety of methods.\n```{r}\nplot(rhohat(bei,Z))\n```\n\nAn important class of covariates are distance maps. Distance maps record the distance in the region from some important features such as water features, intake locations, fault lines, etc. GIS packages have tools to create these but *spatstat* can do it as well.\n\n```{r}\ndata(copper) # location of copper finds and covariate data\n\nX <- rotate(copper$SouthPoints, pi/2) # rotate the points\nL <- rotate(copper$SouthLines, pi/2) # rotate the fault lines\n\nplot(X, pch = 16, main = \"Copper Finds and Fault Lines\")\nplot(L, add = TRUE) \nZ <- distmap(L)\nplot(L, lwd = 2, main = '')\ncontour(Z, add = TRUE)\n```\n\nNow apply the $\\rho$ estimate to the distance covariate.\n\n```{r}\nplot(rhohat(X,Z), xlab = 'Z = Distance to Nearest Fault')\n```\n\n##Testing for Complete Spatial Randomness (CSR)\n\nWe have described $\\lambda(s)$ using several methods. But we must be able to quantify the variable insensity and determine if it is reasonable random or truly caused by some spatial process or covariate. We do this by testing the intensity of the point pattern against a concept sometimes called Compete Spatial Randomness (CSR). For a process to be CSR - that is driven by a *uniform Poisson point process* (we demonstrated this in class) with constant intensity, $\\lambda$, it must meet these conditions...\n\n1.The number of points $n_a$ falling in any subregion $a$ has a Poisson distribution with $\\mu = \\lambda * area(a)$,\n2.given that there are $n_a$ points in $a$, the locations of those points are i.i.d and uniformly distributed inside $a$, and\n3.the contents of two disjoint regions $a$ and $b$ are independent.\n\nCSR is the \"null hypothesis\" and Pearson's Chi-square Goodness of fit test can be used in the quadrat count method (we saw this in the example in class). This is simple in the *spatstat* package using the command *quadrat.test*. \n\nLet's look at this 1st order test of homongenous intensity on three generated ppp datasets. One dataset is intended to follow an homogenous poisson process (the complete spatial random set, or CSR), the second dataset is generated using a Matern process which tends to generate clustered point patterns, and the third data uses a Simple Sequential Inhibition (SSI) process to generate a regular point pattern.\n\n```{r}\nthis.window = owin(c(0,10),c(0,10))\npp.csr <- rpoispp(1, win=this.window) # intensity ~= 1\npp.cluster <- rMatClust(1,0.2,1,win=this.window) # intensity ~= 1\npp.regular <- rSSI(0.75,100, win=this.window) # intensity = 1 (100 points in 100 units)\n```\n\nThe command *quadrat.test* uses Pearson's $\\chi^2$ statistic, $$ \\chi^2 = \\sum_{i=1}^{k}[(n_{i,observed} - n_{i,expected})^2/n_{i,expected}]$$ where $n_{i,observed}$ = number of points in quadrat $i$, $n_{i,expected}$ is the number of points in quadrat $i$ predicted by CSR. $n_{i,expected}$, and $k$ is equal to the number of quadrats. \n\nWe compare the $\\chi^2$ test statistic to the $\\chi^2$ distribution with $k - 1$ degrees of freedom. By default, the two-sided p-value is computed, so that the test will be declared significant if $\\chi^2$ is either very large or very small. One-sided p-values can be obtained by specifying the alternative. An important requirement of the $\\chi^2$ test is that the expected counts in each quadrat be greater than 5.\n\n```{r}\ncsr.test <- quadrat.test(pp.csr,nx=4,ny=4, method = 'Chisq') # Chisq is the default method\nplot(pp.csr, pch='+', color='black')\nplot(csr.test, add=TRUE,  col=\"red\", cex=1.4, lty=2, lwd=3) #pretty plot of the test\ncsr.test # print the test results\n```\n\nNote that by plotting the *h.test* object returned from the command, we can see both $n_{i,observed}$ (top left) and $n_{i,expected}$ (top right) in each $i$ quadrat. We also see the contribution of that quadrat to the $\\chi^2$ sum in the form of the Pearson resdiual, $Pearson_residual = (n_{i,observed} - n_{i,expected})/\\sqrt{n_{i,expected}}$.  \n\nNow, try the clustered pattern.\n\n```{r}\ncluster.test <- quadrat.test(pp.cluster, nx=4, ny=4, method = 'Chisq')\nplot(pp.cluster, pch='+', color='black')\nplot(cluster.test, add=TRUE, col=\"blue\", cex=1.4, lty=2, lwd=3) #pretty plot of the test\ncluster.test # print the test results\n```\n\nAnd, finally the regular pattern.\n\n```{r}\nregular.test <- quadrat.test(pp.regular, nx=4, ny=4, method = 'Chisq')\nplot(pp.regular, pch='+', color='black')\nplot(regular.test, add=TRUE, col=\"green\", cex=1.4, lty=2, lwd=3) #pretty plot of the test\nregular.test # print the test results\n```\n\nNote the large p-values in the clustered and regular patterns, indicating that these are unlikely outcomes of a uniform Poisson process. \n\nWe can easily run the chi-sqare test on real data.\n```{r}\nbei.test <- quadrat.test(bei, nx=6, ny=3, method = 'Chisq')\nplot(bei, pch='.', color='black')\nplot(bei.test, add=TRUE, col=\"blue\", cex=1.4, lty=2, lwd=3) #pretty plot of the test\nbei.test # print the test results\n```\n\nWe may also use the Monte Carlo method rather that the $\\chi^2$ test. This can be useful if the expected quadrat counts are less than 5. If *method = 'MonteCarlo'* is used then command generates a number (given by parameter *nsim*) of random point patterns with the same region-wide intensity as the point cloud. The $\\chi^2$ value is computed then computed using the actual quadrat counts from these CSR patterns as $n_{i,expected}$. The two-sided test is then performed.\n\n```{r}\nbei.test <- quadrat.test(bei, nx=6, ny=3, method = 'MonteCarlo',nsim=100)\nplot(bei, pch='.', color='black')\nplot(bei.test, add=TRUE, col=\"red\", cex=1.4, lty=2, lwd=3) #pretty plot of the test\nbei.test # print the test results\n```\n\n\nThe high p-value indicates we should reject the null-hypothesis of CSR. Note that this doesn't tell us anything about the variable intenstiy (except the changes by quadrat), only that it doesn't appear to be random.\n\nWe can use covariates as well in *quadrat.test*, in this case to define irregular quadrats based on the tesselation of the terrain slope in the region.\n\n```{r}\n# we did all this earlier, but here it is again\nZ <- bei.extra$grad\nb <- quantile(Z, probs=(0:4)/4)\nZcut <- cut(Z, breaks=b,labels=c('low','mlow','mhigh','high'))\nV <- tess(image = Zcut)\n\n# perform the test using the slope tesselation and the Chisq method \nbei.slope.test <- quadrat.test(bei,tess=V, method = 'Chisq')\nplot(bei.slope.test)\nbei.slope.test\n```\n\nThis shows a marked variation across the slope regions we defined. The choice of quadrat size and shape is critical in these tests and must be chosen carefully.\n\nA more powerful test is the Kolmogorov-Smirnov test. The KS test compares the observed and expected distributions of the value of some function $T(s)$ defined at all locations $s$ in the window (ie. region). We evaluate $T(s_j)$ for point $j$ in the pattern. So, for example, we could define $T(grad(s))$ where $grad(s)$ is the value of the gradient at point $s$. In this case, KS is comparing the distribution of slope values at our points with the distribution of all the slope values across the region. The function *cdf.test* implements several cumulative distribution tests on point patterns including the Kolmogrov-Smirnov test. \n```{r}\nbei.slope.ks <- cdf.test(bei,Z,test=\"ks\")\nplot(bei.slope.ks)\n```\n\nThe plot of the returned object shows the expected distribution of slope values (the cumulative probability a seeing a slope less than Z on the x-axis) in red and the observed cumulative probability of only values at the tree locations. Note that in this case the likelihood of finding trees on slopes less than about 0.20 is lower than we would expect given the distrubion of slope values in the region. \n\n```{r}\nbei.slope.ks\n```\n\nThe test results again show the improbability of this not be related to slope. \n\nThe KS test is preferred if the covariate is continuous (like slope). If the covariate is a discrete variable (like landcover type) then KS is ineffective because of tied values and the quadrat test using $\\chi^2$ is preferred. More information on this might be a good project.\n\n## Using spatial logistic regression\n\nBefore we talk about modeling variable intensity in a point pattern, note another method often taught in spatial science classes, the spatial logist regression model. You can keep this in mind as we move forward.\n\n```{r}\ndata(copper)\nX <- rotate(copper$SouthPoints, pi/2)\nL <- rotate(copper$SouthLines, pi/2)\nD <- distfun(L)\nfit <- slrm(X ~ D)\nfit\n```\n\nThe fit is $log(p/(p-1)) = intercept + D(s)*X(s)$ where $s$ is the location and $p$ is the probability of finding an event at that location with the \"fit\" *slrm* object there are methods such as predict that allow you predict the probabilty of an event at a particular location, $s$, given the value of the covariate (in this case, the locations distance from a fault line).\n\n```{r}\npredicted.copper <- predict(fit)\nplot(predicted.copper)\nplot(X, add=TRUE, pch='+')\n```\n\nWe'll look at this more later.\n\n## Modeling defined inhomogenous or non-stationary Poisson processes\n\nIf we assume that no interaction (second order effects) are at work, we can begin to create maximum likliehood models of intensity variations inhomogenous Poisson processes just as we do in other model fitting. The intensity variations can be modeled a function of location in the region or as a function of discrete and/or continuous covariates, or both.\n\nBaddeley provides a little theory in Section 15 of the CSIRO workshop. We'll not discuss that too mucher more here but concentrate instead on the implementation and interpretation of results. \n\nLet's first consider the case where the intensity of the point pattern is a function of $s$. We can create a random pattern with intensity increasing as you move away from a defined point $s_o$ in or out of the region, for example.\n\n```{r}\nlambda <- function(x,y,xo,yo) {exp(0.1*(x-xo)^2 + 0.1*(y-yo)^2)}\nradial.intensity <- rpoispp(lambda,win=owin(c(0,10),c(0,10)),xo=5,yo=5)\nplot(radial.intensity)\n```\n\nAs expected, the intensity increases with log of squared distance from the point $s_o = (5,5)$. The goal of MLE is to use the observed point pattern itself to try to estimate parameters of the model. In this case we define the model as increasing intensity as the square of the radius from a point. The parameters we might try to estimate are $x_o$ and $y_o$. Note that we'll always define the logarithm of the intensity. Since the intensity function is non-linear in $x$, $y$, $xo$ and $yo$ this is rather hard to model. It can be done but let's look a function that is log-linear in $x$ and $y$. Assume that our theoretical (ie. physical) model is that the log-intensity increases as we move away from the lower-left corner of the region. Then $\\lambda_\\theta((x,y)) = exp(\\theta_o + \\theta_1 x + \\theta_2 y)$. Our goal, just as in any regression model is to estimate the $\\theta$'s. The MLE estimate (there is no easy way to do this analytically) is computed using the Berman-Turner algorithm. It's not necessary to understand the mechanics, only the purpose and outcome. \n\nLet's generate a pattern with known parameters and see if we can estimate them.\n\n```{r}\n# define lambda function with vector theta\nlambda <- function(x,y,theta) {\n  exp(theta[1] + theta[2]*x + theta[3]*y) \n}\nknown.intensity <- rpoispp(lambda,win=owin(c(0,10),c(0,10)),theta=c(0.1,0.5,0.25))\nplot(known.intensity)\n\n```\n\nNote that by our definition of $\\theta$ the intensity has a base intensity of 1 and increases more in the x-direction and than in the y (by a factor of 2). Let's see if we can accurate estimate these parameters. The modeling workhorse in *spatstat* is *ppm* (for point pattern model). It follows the same syntax and conventions as the other modeling tools in *R*.  \n\nThe *ppm* function takes as an argument the *ppm* object and a function (or as we'll see later *im*, *tess* and other objects).  The function defines the log (that's important!) of the intensity trend. \n\n```{r}\nppm(known.intensity~x+y)  # The form we're using here is how linear functions can be quickly defined in R using the ~ operator. By default, R assumes that the constant value is included. This is read as known.intensity as a function of x + y\n```\n\nNote the estimates. Did our \"true\" values fall within the (rather large) 95% confidence intervals of the parameters?  This the essence of modeling non-stationary intensity in point patterns. You must always define a parameterized model that predicts intensity as a function of location. \n\nCan you predict or interpret the estimate obtained by the following call to *ppm*?\n\n```{r}\nppm(known.intensity~1)\n```\n\nYou should try this with other examples to get comfortable with the function notation and what it means. Remember the log-linear relationship of $\\lambda$ to $x$ and $y$.\n\n## Covariates as model components\n\nIt is also possible to fit an inhomogeneous Poisson process model with an intensity function\nthat depends on an observed covariate. Let $Z(s)$ be a covariate that has been measured at\nevery location $s$ in the study window. Then $Z(s)$, or any transformation of it, can serve as the\nstatistic $S(s)$ in the parametric form of the intensity function. $log\\lambda_\\theta(s) = \\theta S(s)$ where $S(s)$ is a vector-valued function of location $s$.\n\nSince we have covariates for the *bei* dataset let's start with it. We've seen in the previous sections that there is evidence linking the intensity of trees to slope. We may model this explicitly as $\\lambda(s) = exp(\\beta_0 + \\beta_1 Z(s))$ where again, $Z(s)$ is the slope at location $s$ and $\\beta_0$ and $\\beta_1$ are the parameters to be estimated. The model setup in *spatstat* is\n\n```{r}\nslope <- bei.extra$grad\nppm(bei, ~slope, covariates = slope)  # note the use of ~ again to define a formula or function of slope and the assignment of grad to slope as the covariates\n```\n\nThe fitted model is thus $\\lambda(s) = exp(-5.390553 + 5.022021 Z(s))$.\n\nWhat if we wanted the intensity of proportional to slope as in $\\lambda(s) = \\beta Z(s)$? Then, because we are dealing with log of the intensity we take the log of both sides to get our formula as in $log\\lambda(s) = log\\beta + logZ(s)$. This gets a little tricky to model since second term has no coefficient. It's just an offset. We can model it this way...\n\n```{r}\nppm(bei, ~offset(log(slope)), covariates = slope)\n```\n\nHere our fitted model becomes $\\lambda(s) = e^{-2.427127} Z(s) = 0.0883 Z(s)$.\n\nThe next step in any modeling exercise is to check the fit. The function *ppm* returns a fitted model object of type also *ppm* which we can use to assess the quality of the model. Page 100 of the CSIRO workshop has functions that understand fitted pp models. Lets compare our two models for bei intensity relative to slope.\n\n```{r}\nbei.lin.fit <- ppm(bei, ~slope, covariates = slope) # the exponential fit\nbei.exp.fit <- ppm(bei, ~offset(log(slope)), covariates = slope) # the linear fit\n```\n\nLet's create a prediction of intensity and the standard error (difference between the actual intensity and the predicted intensity) across the window for the exp fit first.\n\n```{r}\nbei.exp.pred <- predict(bei.exp.fit, se = TRUE) \nbei.exp.pred\n```\n\nNote that this produces two pixel arrays 128 x 128. This is the grid that the fit produced and at which predictions were made and compared to actual local intensity. The *plot* knows how to render these as images.\n\n```{r}\nplot(bei.exp.pred$se, main='Standard error of fitted intensity, exponential model')\n```\n\nJust as with residuals in a linear model (WLESS), if you see patterns in the residuals, it means you may not have modeled correctly or completely. Let's try the lin.fit...\n\n```{r}\nbei.lin.pred <- predict(bei.lin.fit, se = TRUE) \nbei.lin.pred\nplot(bei.lin.pred$se, main='Standard error of fitted intensity, linear model')\n```\n\n## Model selection\n\nHow do we intelligently choose which model is better? One often used method is the Akaike Information Criterion, or more commonly AIC. In general, the smaller the AIC, the more \"efficient\" the model. Efficiency is thought as the number of terms required versus the stardard error. \n\n```{r}\nAIC(bei.lin.fit)\nAIC(bei.exp.fit)\n```\n\nThe linear fit is only slightly better than the exponential fit. It is important to note other methods as well such as analysis of deviance. This particular method can only compare hierarchical models. For example, if you have model that varies with $x$ and one the varies with $x$ and $y$, then the first model is considered a \"sub-model\" of the second. In this case the function *anova* can be used. See page 103 of the CSIRO workshop for a few more details. \n\n## Model simulation\n\nOnce we have identified one or models, we may use that model to simulate new point patterns using the *rmh* function. \n\n```{r}\nX <- rmh(bei.exp.fit) \nplot(X)\n```\n\n## Model checking\n\nIt is typical to \"check\" a fitted model for agreement with the data. The check can be a formal hypothesis test ($\\chi^2$ tests, goodness-of-fit test, Monte Carlo tests) or Bayesian model selection (another good project idea). Or the check can be informal and visual. Simply viewing plots of residuals can inform the interpretation and analysis of the model.\n\n### $\\chi^2$ goodness-of-fit\n\nWe've used *quadrat.test* on the point pattern itself, but we can also use it on a *ppm* fit. Under the null hypothesis, the quadrat counts are independent Poisson variables with different mean values, and the means are estimated by the fitted model. \n\n```{r}\nM <- quadrat.test(bei.exp.fit, nx = 8, ny = 4)\nM\n```\n\nHere we've used square quadrats but that really doesn't make sense. Let's try it with the actual tesselation to see if that's any better.\n\n```{r}\nM.tess <- quadrat.test(bei.lin.fit,tess=V, method = 'Chisq')\nM.tess\n```\n\nAnd the $\\chi^2$ statistic is much smaller.\n\nThe plot produced from this object is similar to the prior plots but instead of comparing the observed intensity to the homogenous intensity, it is comparing the observed intensity to the model predicted intensities in those quadrats (the *$estimate* from before). Large Pearson residuals are gross departures from the model. \n\n```{r}\nplot(M.tess)\n```\n\nWe may also use the *cdf.test* as before with the fitted model and the covariate.\n\n```{r}\ncdf.test(bei.exp.fit,slope,test=\"ks\") # using slope as the appropriate covariate (the one used in the model)\n```\n\n###Residual inspection\n\nResiduals from the fitted model are an important diagnostic tool in all areas of applied statistics and can be applied to point pattern intensity analysis as well. For a point pattern with fitted intensity $\\hat{\\lambda}(s)$ the predicted number of points falling in any region $B$ is $\\int_B \\! \\lambda(s) \\, \\mathrm{d}s$. For any given region $B$ then, the residual is defined as the number of *observed* points minus the number of *predicted* points. These are of course closely related to the Pearson residuals in the quadrat tests in the previous sections when $B$ is one of the quadrats. \n\nThe preferred method to view the calculate and view the residuals is to compare the fitted model predictions of intensity to an appropriately scaled KDE image of the real data. This is fully implemented in the following function.\n\n```{r}\ndiagnose.ppm(bei.exp.fit, which = \"smooth\", sigma=100)  # exponential file visualized\n\ndiagnose.ppm(bei.lin.fit, which = \"smooth\", sigma=100)  # linear (or proportional fit visualized)\n```\n\nThis is a powerful function with many options.  The *Lurking variable* plot is a technique worth investigating (another idea for a project). For example, if we wanted to display the residuals against a covariate (slope eg.) then we would plot $C(z) = R(B(z))$ against $z$ where $B(z) = \\{u \\in W : Z(s) \\leq z\\}$ is the region of space where the covariate value is less than or equal to $z$.\n\n```{r}\nlurking(bei.exp.fit, slope, type = \"raw\")\n```\n\nThere is much more to do here but you most, if not all, the tools you'll need to get started analyzing intensity. We'll move on to second order effects now.\n\n#Interaction (second order effect)\n\nThis part part of the analysis is tricky because all these techniques assume that clustering or regularity is due NOT to intensity variations in the process, but rather to interactions among the events themselves - be careful here because you have to make an a pretty big leap and a good argument for homogeneity (or stationarity). It is important that you thoroughly investigate first order effects first. Your understanding of intensity variations across the space will inform this part of the analysis. How to discern intensity variations and interaction effects is a key to point pattern analysis and general very difficult.\n\n##Data exploration and \"old school\" but useful plotting techniques\n\nData exploration first (as always). First, let's generate some point patterns 1) a true CSR pattern, 2) a clustered pattern and 3) regular pattern (as we did earlier when looking at intensity).\n\n```{r}\nthis.window = owin(c(0,10),c(0,10))\npp.csr <- rpoispp(1, win=this.window)\npp.cluster <- rMatClust(1,0.2,3,win=this.window)\npp.regular <- rSSI(0.75,100, win=this.window)\n```\n\nThere are two simple and often used methods for viewing distances between point pairs. Note that if a pattern has $n$ points then it will have $(n^2)/2-n$ unique interactions (half of the distance matrix minus the zero-valued diagonals).\n\nThe Morishita plots is multi-scalar plot that shows chi-squared results at various quadrat sizes. The Morishita index (first published in 1959), $M_i$, is computed as $$M_i = Q * \\sum_{n=1}^{Q} (n_i(n_i - 1))/(N(N-1))$$ where $Q$ is the number of quadrats, $n_i$ is the number of points falling in quadrant $i$ and $N$ is the total number of points. If the pattern is completely random then $M_i ~= 1$. Values greater than 1 suggest clustering at that scale.\n\nThe Morishita plot is plot of $M_i$ against the linear dimension of the quadrats. The point pattern window is divided first into $2 x 2$ quadrats, then $3 x 3$ quadrats, and so on, with $M_i$ computed each time. \n\nSo, we can regenerate our three patterns (csr, clustered and random) to see how the Morishita plot responds to each.\n```{r}\nmiplot(pp.csr)\nmiplot(pp.cluster)\nmiplot(pp.regular)\n```\n\nNotice in particular the multi-scalar aspect of these plots. If we look at the plot of pp.cluster we see less clustering as the size of the quadrat increases.  At about 3 units, no clustering is discernable. That clustering descreases as we increase quadrat size should be intuitive but it is useful to know at what scale that happens. We will see this multi-scalar approach later in the K statistic.\n\nHow do the hickory trees in the lansing dataset plot?\n\n```{r}\ndata(lansing)\nmiplot(split(lansing)$hickory)\n```\n\nYou should be able to look at this plot and see at what scale clustering ends.\n\nAnother (less useful) graphing method is the Fry plot. The simplest way to explain this is not with a formula but with a graphic description. Assume you've plotted your point patter on a piece of paper. Take a transparent page and draw a red dot in the center. Place the red dot on the transparency at an event and mark all the other events on the transparent page. Move the red dot to the next event and repeat. Do this for for all events. The result is the fry plot.\n\nOnce again, let's see how our three patterns plot.\n\n```{r}\nfryplot(pp.csr,char=0.01,pch='.')\nfryplot(pp.cluster,char=0.01,pch='.')\nfryplot(pp.regular,char=0.01,pch='.')\n```\n\n##Distance based functions                   \n\nAs useful as the Morishita and Fry plots can be for getting a sense of 2nd order effects in our pattern, they nevertheless are somewhat qualitative and difficult to interpret. More sophisticated distance-based methods have been developed in an attempt to better assess second order effects in point patterns. Note that when we speak of distances we are generally talking about Euclidean distances. However, other distance measures are possible (Manhattan, Mahalanobias, etc.). We use  $|s_i - s_j|$ to denote a distance (no matter how distance is calculated) between points $s_i$ and $s_j$).\n\nDistance functions generally require one of three possible distance calculations:\n\n1. **Pairwise distance** is the distance between all distinct pairs of points $s_i$ and $s_j$ $\\forall i,j$ in the point pattern.\n\n```{r}\nD.pairwise <-  pairdist(pp.csr, squared=FALSE, periodic=TRUE) #returns a matrix\n```\n\nNote also that *crossdist* is available which compete distances between points with different marks or different point pattern sets. This results in a generally non-square matrix. \n\n```{r}\nhickory <- split(lansing)$hickory # separate hickory trees\nmaple <- split(lansing)$maple # separate maple trees\nD.cross <- crossdist(hickory,maple)\n```\n\n2. **Nearest neighbor distance** is defined as $t_i = \\min(|s_i - s_j|)  \\forall j$ so that $t_i$ is the distance from each point $s_i$ to it's nearest neighbor.\n\n```{r}\nD.nn <- nndist(pp.csr, k=1) # returns a vector\nD.nn[1:5]  # the distance from the nearest point to the first five points in the ppp object\n```\n\n3. **Empty space distance** is defined as $d_u = \\min(|u - s_i|) \\forall i$ where $d_u$ is the distance from a fixed reference location, $u$ to the nearest point in the pattern. Typical the reference locations are defined on a grid. This is generated by a call to the *distmap* function which saw earlier developing a covariate (distance from a fault line in our example).\n\n```{r}\nD.esd <- distmap(pp.csr)\nplot(D.esd, main = \"Empty Space Distances\")\nplot(pp.csr, add=TRUE)\n```                \n\nNote that none of these basic distance functions are edge corrected. Later we'll see functions which are edge corrected.\n\n                   \n## Comparing Empty Space Distances (the $F$ function)\n                     \nThe $F$ function is a measure of the distribution of all empty space distance in a point pattern. \n\n$$\\hat{F}(r) = \\sum_{k=1}^{m} I_k/m$$ \n\n$$I_k = \n\\left\\{\n  \\begin{array}{ll}\n\t\t1 & \\mbox{if } d_k \\in {d_k:d_k \\leq r, \\forall k} \\\\\n\t\t0 & \\mbox{otherwise } \n\t\\end{array}\n\\right.$$\n\nwhere $d_k = \\min_{j}(d_{kj}, \\forall j \\in S), k = 1,\\ldots,m, j = 1,\\ldots,n$\n\nIt can be shown that a CSR pattern with intensity $\\lambda$ would yield $F_{r,csr} = 1 - e^{\\lambda\\pi r^2}$. So we can plot $F_r$ against $r$ itself.\n\n```{r}\nlambda <- summary(bei)$intensity  # extract the intensity of the bei dataset\nF.csr <- function(r) {1 - exp(-lambda*pi*r^2)} # create a function to compute F with at that intensity and at a distance, r\ncurve(F.csr,0,10) # draw a curve of F computed at r = 0, 1, ..., 9, 10\n```                \n\nThe *spatstat* function, *Fest* offers a convenient to compute *F_r* for a point pattern along with $F_{r,csr}$ in a single command. For example, for the *bei* point pattern of 3604 points in 1000 x 500 meter window...\n\n```{r}\nF.bei <- Fest(bei)                     \nplot(F.bei)\n```\n\nThis produces several estimates of $F_r$ and compares them $F_{r,csr}.\n\n```{r}\nplot(F.bei, theo ~ r, main = 'Theoretical')\nplot(F.bei, rs ~ r, main = 'border corrected') \nplot(F.bei, cbind(rs, theo) ~ r)\n```\n\nThese are obviously similar to expected F for a CSR\n                     \nlet's see how different patterns behave                     \n```{r}\nplot(Fest(pp.csr),rs ~ r, lty=2)\nplot(Fest(pp.regular), rs ~ r, add=TRUE, col='blue')\nplot(Fest(pp.cluster), rs ~ r, add=TRUE, col='red')\n```\n\n NEAREST NEIGHBOR DISTANCES (the G function)\n\nThe cumulative distribution function G(r) = P{d(u,X \\ {u}), u is an element of X}\n for a homongenous Poisson process Gpois(r) = 1 - exp(-lambda*pi*r^2)  (same as F, right?)\n```{r}\nGbie <- Gest(bei)\nplot(Gbie)                     \n```                     \n\nIf events are closely clustered space, G increases rapidly at short distances\n```{r}\nG.cluster <- Gest(pp.cluster)\nplot(G.cluster)\n```\n\nWhat happens if event are more regular\n```{r}\nG.regular <- Gest(pp.regular)                     \nplot(G.regular)\n```                     \n \n PAIRWISE DISTANCES (the K function)\n the observed distances in a point pattern constitute a biased sample of pairwise distances\n in the process, favoring smaller distances (since we'll never observe a distance large than the window)\n\nRipley defined the K-function so that lambda*K(r) is the expected number of points in the process within a distance r of a typical point of the process\n Kpois = pi * r^2 (it doesn't depend on the intensity)\n```{r}\nK.csr <- Kest(pp.csr)\nplot(K.csr)    \n```                     \n\nComputing envelopes using Monte Carlo techniques (Baddelley, page 132)\n```{r}\nK.regular.env <- envelope.ppp(pp.regular,Kest,nsim=39,rank=1)                     \nK.cluster.env <- envelope.ppp(pp.cluster,Kest,nsim=39,nrank=1)\nK.csr.env <- envelope.ppp(pp.csr, Kest, nsim=39, nrank=1)\n```                     \n\nNote that Kest can be border/edge corrected. See ?Kest\n         \n another commonly used statistic the transformed K value, L, of course.\n L(r) = sqrt(K(r)/pi) which makes a L of a poisson process a straight line\n\n```{r}\nL.csr <- Lest(pp.csr)\nplot(L.csr)\n```\n\n```{r}\nL.cluster <- Lest(pp.cluster)\nplot(L.cluster)\n```                     \n\nAlso the J function J(r) = (1 - G(r))/(1-F(r))  so that Jpois(r) = 1\nJ(r) > 1 suggest regularity, J(r) < 1 suggest clustering\n\nyou could just do this...\n```{r}\nplot(allstats(bei))\n```                     \n                     \n CAVEATS TO ALL OF THE PRECEDING\n 1. F,G, and K are defined and estimated under the assumption that the point process is homogenous\n\n 2. summary functions do not characterize the process (that's model fitting)\n\n 3. if the process is not homogenous, deviations between thereotical and empirical are not evidence (necessarily) of interpoint interaction, since they may also be attributable to variations in intensity\n                     \n for example, #3\n```{r}\nX <- rpoispp(function(x,y){300*exp(-3*x)})\nX\n```                     \n                     \n                     \n                     \n                     \n                     \n                     \n                     ",
    "created" : 1423762362432.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2177669077",
    "id" : "F50D4F74",
    "lastKnownWriteTime" : 1423762501,
    "path" : "~/GitHub/stats_spring_2015/Point_Pattern_R Notes.Rmd",
    "project_path" : "Point_Pattern_R Notes.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}