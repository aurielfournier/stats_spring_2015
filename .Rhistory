scmsu1.p <-as.data.frame(spsample(scmsu1, n=20, type='random', iter=10))
scmsu2.p <- as.data.frame(spsample(scmsu2, n=20, type='random', iter=10))
scmsu3.p <- as.data.frame(spsample(scmsu3, n=20, type='random', iter=10))
scsge.p <- as.data.frame(spsample(scsge, n=20, type='random', iter=10))
scsgd.p <- as.data.frame(spsample(scsgd, n=20, type='random', iter=10))
scsgc.p <- as.data.frame(spsample(scsgc, n=20, type='random', iter=10))
scsgb.p <- as.data.frame(spsample(scsgb, n=20, type='random', iter=10))
scnmallard.p <- as.data.frame(spsample(scnmallard, n=20, type='random', iter=10))
fgwalkin3.p <- as.data.frame(spsample(fgwalkin3, n=20, type='random', iter=10))
fgpool3.p <- as.data.frame(spsample(fgpool3, n=20, type='random', iter=10))
fgwalkin2.p <- as.data.frame(spsample(fgwalkin2, n=20, type='random', iter=10))
fgpool2.p <- as.data.frame(spsample(fgpool2, n=20, type='random', iter=10))
fgpool1.p <- as.data.frame(spsample(fgpool1, n=20, type='random', iter=10))
dc15.p <- as.data.frame(spsample(dc15, n=20, type='random', iter=10))
dc10.p <- as.data.frame(spsample(dc10, n=20, type='random', iter=10))
dc8.p <- as.data.frame(spsample(dc8, n=20, type='random', iter=10))
dc13.p <- as.data.frame(spsample(dc13, n=20, type='random', iter=10))
dc14.p <- as.data.frame(spsample(dc14, n=20, type='random', iter=10))
dc20.p <- as.data.frame(spsample(dc20, n=20, type='random', iter=10))
dc16.p <- as.data.frame(spsample(dc16, n=20, type='random', iter=10))
dc23.p <- as.data.frame(spsample(dc23, n=20, type='random', iter=10))
dc33.p <- as.data.frame(spsample(dc33, n=20, type='random', iter=10))
dc21.p <- as.data.frame(spsample(dc21, n=20, type='random', iter=10))
dc11.p <- as.data.frame(spsample(dc11, n=20, type='random', iter=10))
dc18.p <- as.data.frame(spsample(dc18, n=20, type='random', iter=10))
ts2a.p <- as.data.frame(spsample(ts2a, n=20, type='random', iter=10))
ts4a.p <- as.data.frame(spsample(ts4a, n=20, type='random', iter=10))
ts6a.p <- as.data.frame(spsample(ts6a, n=20, type='random', iter=10))
ts8a.p <- as.data.frame(spsample(ts8a, n=20, type='random', iter=10))
slm10.p <- as.data.frame(spsample(slm10, n=20, type='random', iter=10))
slm11.p <- as.data.frame(spsample(slm11, n=20, type='random', iter=10))
slm13.p <- as.data.frame(spsample(slm13, n=20, type='random', iter=10))
slm5.p <- as.data.frame(spsample(slm5, n=20, type='random', iter=10))
slm4.p <- as.data.frame(spsample(slm4, n=20, type='random', iter=10))
slm3.p <- as.data.frame(spsample(slm3, n=20, type='random', iter=10))
bkkl9.p <- as.data.frame(spsample(bkkl9, n=20, type='random', iter=10))
bkkl5.p <- as.data.frame(spsample(bkkl5, n=20, type='random', iter=10))
bkkl2.p <- as.data.frame(spsample(bkkl2, n=20, type='random', iter=10))
bkkl6.p <- as.data.frame(spsample(bkkl6, n=20, type='random', iter=10))
tmpC.p <- as.data.frame(spsample(tmpC, n=20, type='random', iter=10))
tmpE.p <- as.data.frame(spsample(tmpE, n=20, type='random', iter=10))
tmpI.p <- as.data.frame(spsample(tmpI, n=20, type='random', iter=10))
nvsanc.p$ID<- paste("nvsanc",1:20,sep="_")
nvash.p$ID<- paste("nvash",1:20,sep="_")
nvred.p$ID<- paste("nvred",1:20,sep="_")
os23.p$ID<- paste("os23",1:20,sep="_")
os22.p$ID<- paste("os22",1:20,sep="_")
os21.p$ID<- paste("os21",1:20,sep="_")
osr7.p$ID<- paste("osr7",1:20,sep="_")
osr6.p$ID<- paste("osr6",1:20,sep="_")
osr3.p$ID<- paste("osr3",1:20,sep="_")
osr1.p$ID<- paste("osr1",1:20,sep="_")
osr9.p$ID<- paste("osr9",1:20,sep="_")
osr45.p$ID<- paste("osr45",1:20,sep="_")
scmsu1.p$ID<- paste("scmsu1",1:20,sep="_")
scmsu2.p$ID<- paste("scmsu2",1:20,sep="_")
scmsu3.p$ID<- paste("scmsu3",1:20,sep="_")
scsge.p$ID<- paste("scsge",1:20,sep="_")
scsgd.p$ID<- paste("scsgd",1:20,sep="_")
scsgc.p$ID<- paste("scsgc",1:20,sep="_")
scsgb.p$ID<- paste("scsgb",1:20,sep="_")
scnmallard.p$ID<- paste("scnmallard",1:20,sep="_")
fgwalkin3.p$ID<- paste("fgwalkin3",1:20,sep="_")
fgpool3.p$ID<- paste("fgpool3",1:20,sep="_")
fgwalkin2.p$ID<- paste("fgwalkin2",1:20,sep="_")
fgpool2.p$ID<- paste("fgpool2",1:20,sep="_")
fgpool1.p$ID<- paste("fgpool1",1:20,sep="_")
dc15.p$ID<- paste("dc15",1:20,sep="_")
dc10.p$ID<- paste("dc10",1:20,sep="_")
dc8.p$ID<- paste("dc8",1:20,sep="_")
dc13.p$ID<- paste("dc13",1:20,sep="_")
dc14.p$ID<- paste("dc14",1:20,sep="_")
dc20.p$ID<- paste("dc20",1:20,sep="_")
dc16.p$ID<- paste("dc16",1:20,sep="_")
dc23.p$ID<- paste("dc23",1:20,sep="_")
dc33.p$ID<- paste("dc33",1:20,sep="_")
dc21.p$ID<- paste("dc21",1:20,sep="_")
dc11.p$ID<- paste("dc11",1:20,sep="_")
dc18.p$ID<- paste("dc18",1:20,sep="_")
ts2a.p$ID<- paste("ts2a",1:20,sep="_")
ts4a.p$ID<- paste("ts4a",1:20,sep="_")
ts6a.p$ID <-paste("ts6a",1:20,sep="_")
ts8a.p$ID<- paste("ts8a",1:20,sep="_")
slm10.p$ID<- paste("slm10",1:20,sep="_")
slm11.p$ID<- paste("slm11",1:20,sep="_")
slm13.p$ID<- paste("slm13",1:20,sep="_")
slm5.p$ID<- paste("slm5",1:20,sep="_")
slm4.p$ID<- paste("slm4",1:20,sep="_")
slm3.p$ID<- paste("slm3",1:20,sep="_")
bkkl9.p$ID <- paste("bkkl9",1:20,sep="_")
bkkl5.p$ID <- paste("bkkl5",1:20,sep="_")
bkkl2.p$ID <- paste("bkkl2",1:20,sep="_")
bkkl6.p$ID <- paste("bkkl6",1:20,sep="_")
tmpC.p$ID <- paste("tmpC",1:20,sep="_")
tmpE.p$ID <- paste("tmpE",1:20,sep="_")
tmpI.p$ID <- paste("tmpI",1:20,sep="_")
a1 <- rbind(slm3.p, slm4.p)
a2 <- rbind(a1, slm5.p)
a3 <- rbind(a2, slm13.p)
a4 <- rbind(a3, slm11.p)
a5 <- rbind(a4, slm10.p)
a6 <- rbind(a5, ts8a.p)
a7 <- rbind(a6, ts6a.p)
a8 <- rbind(a7, ts4a.p)
a9 <- rbind(a8, ts2a.p)
a10 <- rbind(a9, dc18.p)
a11 <- rbind(a10, dc11.p)
a12 <- rbind(a11, dc21.p)
a13 <- rbind(a12, dc33.p)
a14 <- rbind(a13, dc23.p)
a15 <- rbind(a14, dc16.p)
a16 <- rbind(a15, dc20.p)
a17 <- rbind(a16, dc14.p)
a18 <- rbind(a17, dc13.p)
a19 <- rbind(a18, dc8.p)
a20 <- rbind(a19, dc10.p)
a21 <- rbind(a20, dc15.p)
a22 <- rbind(a21, fgpool1.p)
a23 <- rbind(a22, fgpool2.p)
a24 <- rbind(a23, fgwalkin2.p)
a25 <- rbind(a24, fgpool3.p)
a26 <- rbind(a25, fgwalkin3.p)
a27 <- rbind(a26, scnmallard.p)
a28 <- rbind(a27, scsgb.p)
a29 <- rbind(a28, scsgc.p)
a30 <- rbind(a29, scsgd.p)
a31 <- rbind(a30, scsge.p)
a32 <- rbind(a31, scmsu3.p)
a33 <- rbind(a32, scmsu2.p)
a34 <- rbind(a33, scmsu1.p)
a35 <- rbind(a34, osr45.p)
a36 <- rbind(a35, osr9.p)
a37 <- rbind(a36, osr1.p)
a38 <- rbind(a37, osr3.p)
a39 <- rbind(a38, osr6.p)
a40 <- rbind(a39, osr7.p)
a41 <- rbind(a40, os21.p)
a42 <- rbind(a41, os22.p)
a43 <- rbind(a42, os23.p)
a44 <- rbind(a43, nvred.p)
a45 <- rbind(a44, nvash.p)
a46 <- rbind(a45, bkkl9.p)
a47 <- rbind(a46, bkkl5.p)
a48 <- rbind(a47, bkkl6.p)
a49 <- rbind(a48, bkkl2.p)
a50 <- rbind(a49, tmpC.p)
a51 <- rbind(a50, tmpE.p)
a52 <- rbind(a51, tmpI.p)
afinal <- rbind(a52, nvsanc.p)
points <- SpatialPointsDataFrame(data=afinal,  coords=afinal[,1:2], proj4string=CRS("+proj=utm +zone=32 +datum=WGS84"))
writeOGR(points, driver="ESRI Shapefile", layer="random_2014",dsn="C:/Users/Auriel Fournier/Dropbox/GIS", overwrite_layer=T)
#writeOGR(points, driver="GPX", layer="random_2014", ,dsn="C:/Users/Auriel Fournier/Dropbox/GIS", overwrite_layer=T)
pointsll <- spTransform(points, CRS("+proj=longlat +ellps=WGS84"))
writeOGR(pointsll, dsn="C:/Users/Auriel Fournier/Dropbox/GIS",layer="random14",driver="GPX", overwrite_layer=T)
writeOGR(pointsll, dsn="C:/Users/Auriel Fournier/Dropbox/GIS",layer="random14",driver="GPX", overwrite_layer=T)
require(sjPlot) # load package
# load sample data
data(efc)
# retrieve value and variable labels
variables <- sji.getVariableLabels(efc)
values <- sji.getValueLabels(efc)
# simple frequency table
sjt.frq(efc$e42dep,
variableLabels=variables['e42dep'],
valueLabels=values[['e42dep']])
#In Markdown
`r sjt.df(anova(season), alternateRowColors=TRUE,  no.output=TRUE, describe=FALSE)$knitr`
sjt.df(anova(season), alternateRowColors=TRUE,  no.output=TRUE, describe=FALSE)$knitr
sjt.df(anova(season), alternateRowColors=TRUE,  no.output=TRUE, describe=FALSE)
install.packages("RMark")
library(RMark)
data(dipper)
myexample=mark(dipper)
setwd("~/Documents/stats_spring_2015")
setwd("~/stats_spring_2015")
check = read.csv('BayouMetoQA.csv')
# or you could use the slightly more complicated read.table function
check = read.table('BayouMetoQA.csv',header=TRUE,sep=',')
# examine the resulting data.frame
head(check)
# another, more detailed digest of the data
str(check)
# extract the number of rows and cols to a variable (maybe we'll need it later)
n.obs = nrow(check)
n.attributes = ncol(check)
# add a column to the data.frame containing the delta between the GPS and LiDAR Z's.  NOTE: Negative values indicate the lidar return is higher than the GPS return (we'll maintain these values because there is information here)
check$delta = check$Z-check$LidarZ
head(check)
str(check)
# one of the most effective ways to explore the distribution, called the stem and leaf plot, is now taught in 6th grade
stem(check$delta)		# is this left skewed or right skewed?
# note the one distinctly different value, what about in absolute value terms?
stem(abs(check$delta))		# even more pronounced, sRtill skewed?
# discuss observation or data types: continuous (ordinal,interval,ratio)and categorical (ordinal,nominal/discrete)
#							CREATE TABLES CLASSIFYING OBS BY TYPE
# look at the frequency distribution of the three categorical variables (AKA Factors) - Region, LC, Class
# first by Region
# apply the table functionbar
Region.freq = table(check$Region)
Region.freq
# convert to column format for easier reading
cbind(Region.freq)
# do the same for LC (land cover) type
LC.freq = table(check$LC)
# display as a column, but don't save
cbind(LC.freq)
# we may also look at a joint frequency distribution
joint = table(check$LC, check$Region)
# what kind of data type is "joint"?
is.data.frame(joint)
is.character(joint)
is.vector(joint)
is.matrix(joint)   #yes, with colnames and rownames defined
barplot(LC.freq)
colors = c('black','brown','green','pink','cyan')
barplot(LC.freq,col=colors)
class(check$delta ~ check$LC)
# using the formula datatype for continuous data:
plot(check$delta ~ check$Z)		# creates a scatterplot
# while using the formual with an factor as the independent variable:
plot(check$delta ~ check$LC)	# creates a multi boxplot
plot(check$delta ~ check$Region)
Asphalt.index = check$LC=="Asphalt"
Asphalt.index
check[Asphalt.index,]
Forest.index = check$LC == 'Forest'
Forest.index
check[Forest.index,]
# compare the means elevation delta's. Here we use Asphalt.index to index the rows and the column name "delta" to select the column
mean(check[Asphalt.index,'delta'])
mean(check[Forest.index,'delta'])
# shouldn't there be an easier way of getting summaries by category? of course there is...
tapply(check$delta,check$LC,mean)
cbind(tapply(check$delta,check$LC,mean))
# also try lapply (returns list) and sapply (returns simple string)
#what about variance within the LC type
cbind(tapply(check$delta,check$LC,var))
# or, more intuitively, standard deviation
cbind(sqrt(tapply(check$delta,check$LC,var)))
cbind(tapply(check$delta,check$LC,sd))
# or range
cbind(tapply(check$delta,check$LC,range))
cbind(tapply(check$delta,check$LC,min))
cbind(tapply(check$delta,check$LC,max))
cbind(tapply(check$delta,check$LC,quantile))
# lets look only at absolute differences
cbind(tapply(abs(check$delta),check$LC,sd))
# is there a large difference in mean and variation across flights?
cbind(tapply(check$delta,check$Region,mean))
cbind(tapply(check$delta,check$Region,var))
cbind(tapply(abs(check$delta),check$Region,mean))
cbind(tapply(abs(check$delta),check$Region,var))
# evidently, yes. but perhaps the LC distribution is different. this is a job for ANOVA (later in the semester)
# finally, before we move to analyzing the distributions of the continuous variables, note the very useful summary function
tapply(check$delta,check$Region,summary)
breaks = seq(-0.5,0.15,by=0.125)
breaks
# review and download the data file BayouMetoQA.csv
#first look at the csv file in a text editor, note the header, the commas, missing data, etc.
#							PREPARE AND INSPECT THE DATA
# set working directory (to wherever you put your data file...)
setwd("~/Documents/stats_spring_2015")
# verify working directory
getwd()
# load data from a comma-separated-value formatted file
check = read.csv('BayouMetoQA.csv')
# or you could use the slightly more complicated read.table function
check = read.table('BayouMetoQA.csv',header=TRUE,sep=',')
# examine the resulting data.frame
head(check)
# another, more detailed digest of the data
str(check)
# extract the number of rows and cols to a variable (maybe we'll need it later)
n.obs = nrow(check)
n.attributes = ncol(check)
# add a column to the data.frame containing the delta between the GPS and LiDAR Z's.  NOTE: Negative values indicate the lidar return is higher than the GPS return (we'll maintain these values because there is information here)
check$delta = check$Z-check$LidarZ
head(check)
str(check)
# one of the most effective ways to explore the distribution, called the stem and leaf plot, is now taught in 6th grade
stem(check$delta)		# is this left skewed or right skewed?
# note the one distinctly different value, what about in absolute value terms?
stem(abs(check$delta))		# even more pronounced, sRtill skewed?
# discuss observation or data types: continuous (ordinal,interval,ratio)and categorical (ordinal,nominal/discrete)
#							CREATE TABLES CLASSIFYING OBS BY TYPE
# look at the frequency distribution of the three categorical variables (AKA Factors) - Region, LC, Class
# first by Region
# apply the table functionbar
Region.freq = table(check$Region)
Region.freq
# convert to column format for easier reading
cbind(Region.freq)
# do the same for LC (land cover) type
LC.freq = table(check$LC)
# display as a column, but don't save
cbind(LC.freq)
# we may also look at a joint frequency distribution
joint = table(check$LC, check$Region)
# what kind of data type is "joint"?
is.data.frame(joint)
is.character(joint)
is.vector(joint)
is.matrix(joint)   #yes, with colnames and rownames defined
# or simply query it...
class(joint)
# we can extract data from the table or the data.frame in a convenient way using indexing
check[,'LC']	# land cover classes by observation
check[1,]		# first GPS point and all attributes
check[check$LC=='Asphalt',]		# all attributes of GPS points collected on Asphalt
check[abs(check$delta)>0.18,] 	# all attribures of GPS points differing from Lidar by >18cm
joint['Asphalt',]	# number of Asphalt point
joint[,'North']		# nubmer of points in the North by type, etc.
# you can also generate proportion tables, 1=sum by row, 2=by column, '' by total
prop.table(joint,1)
prop.table(joint,2)
prop.table(joint)
# to save the relative frequencey distribution of the LC types to variable
LC.relfreq = LC.freq / n.obs
#						DISPLAY TABLES GRAPHICALLY
# list the current variables in the workspace
ls()
# enough tables and numbers, create some graphs
# simple barplot
barplot(LC.freq)
colors = c('black','brown','green','pink','cyan')
barplot(LC.freq,col=colors)
# note that the plot command is over-ridden for data.frames depending on inputs if we use the formula data type which is creating using the ~ symbol.  ~ can be read as "desribed by"
class(check$delta ~ check$LC)
# using the formula datatype for continuous data:
plot(check$delta ~ check$Z)		# creates a scatterplot
# while using the formual with an factor as the independent variable:
plot(check$delta ~ check$LC)	# creates a multi boxplot
plot(check$delta ~ check$Region)
# pie charts are, in general, better for showing relative frequency graphically
pie(LC.freq,col=colors)
#						DESCRIBE THE DATA
# the presence of categories implies an expected behavior difference (and we saw that above) so we may summarize continuous variables by category
# to dig a little deeper to better understand what R is doing, create a logical vector for the presence of a particular category
Asphalt.index = check$LC=="Asphalt"
Asphalt.index
check[Asphalt.index,]
Forest.index = check$LC == 'Forest'
Forest.index
check[Forest.index,]
# compare the means elevation delta's. Here we use Asphalt.index to index the rows and the column name "delta" to select the column
mean(check[Asphalt.index,'delta'])
mean(check[Forest.index,'delta'])
# this is called slicing, notice the substantially different means (but is the difference significant given the variation we see? - this is what ANOVA tells us)
# shouldn't there be an easier way of getting summaries by category? of course there is...
tapply(check$delta,check$LC,mean)
cbind(tapply(check$delta,check$LC,mean))
# also try lapply (returns list) and sapply (returns simple string)
#what about variance within the LC type
cbind(tapply(check$delta,check$LC,var))
# or, more intuitively, standard deviation
cbind(sqrt(tapply(check$delta,check$LC,var)))
cbind(tapply(check$delta,check$LC,sd))
# or range
cbind(tapply(check$delta,check$LC,range))
cbind(tapply(check$delta,check$LC,min))
cbind(tapply(check$delta,check$LC,max))
cbind(tapply(check$delta,check$LC,quantile))
# notice that the large variation in scrub, grass and forest is due to a large -/+ range
# lets look only at absolute differences
cbind(tapply(abs(check$delta),check$LC,sd))
# is there a large difference in mean and variation across flights?
cbind(tapply(check$delta,check$Region,mean))
cbind(tapply(check$delta,check$Region,var))
cbind(tapply(abs(check$delta),check$Region,mean))
cbind(tapply(abs(check$delta),check$Region,var))
# evidently, yes. but perhaps the LC distribution is different. this is a job for ANOVA (later in the semester)
# finally, before we move to analyzing the distributions of the continuous variables, note the very useful summary function
tapply(check$delta,check$Region,summary)
# remember to look by at these summary results when discussing skewness and kurtosis
# frequency distribution of quantitative data
# we may reduce typing by attaching "check" to the workspace. we may refer to the columns without the $ notation
attach(check)
# verify the range of the data
range(delta)
# in order to look at frequency distributions, we need to group the continuous variables
breaks = seq(-0.5,0.15,by=0.125)
breaks
# bin, or collect, the delta values according to these sub-intervals, the cut function will define intervals based on breaks and the classify each element in "delta" by the classifications
delta.cut = cut(delta,breaks,right=FALSE)
# note that cut creates factors with length(breaks)-1 levels
delta.cut
# now we can tabulate the delta values by the interval in which they fall using the table function which counts by levels
delta.freq = table(delta.cut)
cbind(delta.freq)
# it is more efficient to use the hist (histogram) function:
# produces a very useful histogram object and can compute breaks using the rules described in BBR ('Scott','Sturges',others)
delta.hist = hist(delta,breaks='Scott',main='GPS - LiDAR')
# look at delta.hist closely...
summary(delta.hist)
str(delta.hist)
# ...and extract midpoints of the intervals
delta.hist$mids
# QUESTION: why are both intensities and densities and why are they the same in our dataset?
# now compute relative frequency distribution
delta.relfreq = delta.freq / n.obs		# use n.obs from earlier computation
cbind(delta.freq,delta.relfreq)			# why is this different from hist densities/intensities? - breaks are different.
# lets compute an empirical quantile plot, using the formula notation
p=seq(0,1,0.01);
r=quantile(delta,probs=p);
plot(r~p,type='l',xlab='percent',ylab='value')
# What happens as breaks are redefined?
# how many breaks should we use? try Scott's rule (page 49 of BBR)
bin.width = 3.5*sd(delta)*nrow(check)^(-1/3)
bin.width
brk.a = seq(-0.5,0.50,by=0.2)
hist(delta,breaks=brk.a,col='blue',main='-0.5 to 0.5 by 0.125 (brk.a)')
# the bin width is slightly smaller than the separation between the two most distance "adjacent" values...
diff(sort(delta))
# if one of these two data points happen to like near a breakpoint, then there will be an empty bin making observation look like an outlier.
brk.b = seq(-0.55,0.8,by=0.125)
hist(delta,breaks=brk.b,col='red',main='-0.55 to 0.8 by 0.125 (brk.b)')
# another seemingly small change has a significant effect on the distribution - whereas brk.a looks symmetric, this distribution looks left skewed
brk.c = seq(-0.426,0.8,by=0.125)
hist(delta,breaks=brk.c,col='green',main='-0.426 to 0.8 by 0.125 (brk.c)')
# it is a simple matter to look at cumulative frequency distributions using the cumsum function
delta.cumfreq = cumsum(delta.freq)
cbind(delta.cumfreq)
# plot the cumulative distribution
cumfreq0 = c(0,cumsum(delta.freq))
plot(breaks,cumfreq0,type='l',main='Cumulative Distribution of Delta')
lines(brk.a,cumfreq0)
# now, it is perhaps more apparent that the cumulative frequency of absolute values would make more sense. try it with absolute values
#we've seen that delta does depend on flight and LC.  does is also depend on position (you would expect yes since the flights are N and S)
#simple scatter plots will help
plot(check$Z,delta,xlab='Elevation',ylab='Delta')
plot(check$X,delta,xlab='Easting',ylab='Delta')
plot(check$Y,delta,xlab='Northing',ylab='Delta')
# finally, let's look at the delta's summarized graphically
boxplot(delta~check$LC)
boxplot(abs(delta)~check$LC)
#quantile calculations in small samples such as this can be deceiving so the IQR results are likewise misleading.  In this case a stripchart is useful for plotting the raw data
stripchart(abs(check$delta) ~ check$LC,method='jitter')
#note the distributions by land cover - the veg classes show more negative values (why?)
boxplot(abs(delta)~check$LC)
stripchart(abs(check$delta) ~ check$LC,method='jitter')
boxplot(delta~check$LC)
boxplot(abs(delta)~check$LC)
check = read.csv('BayouMetoQA.csv')
head(check)
check$delta = check$Z-check$LidarZ
check$delta
stem(abs(check$delta))		# even more pronounced, sRtill skewed?
Region.freq = table(check$Region)
Region.freq
cbind(Region.freq)
# do the same for LC (land cover) type
LC.freq = table(check$LC)
# display as a column, but don't save
cbind(LC.freq)
joint = table(check$LC, check$Region)
joint
barplot(LC.freq)
colors = c('black','brown','green','pink','cyan')
barplot(LC.freq,col=colors)
# note that the plot command is over-ridden for data.frames depending on inputs if we use the formula data type which is creating using the ~ symbol.  ~ can be read as "desribed by"
class(check$delta ~ check$LC)
# using the formula datatype for continuous data:
plot(check$delta ~ check$Z)		# creates a scatterplot
# while using the formual with an factor as the independent variable:
plot(check$delta ~ check$LC)	# creates a multi boxplot
plot(check$delta ~ check$Region)
# pie charts are, in general, better for showing relative frequency graphically
pie(LC.freq,col=colors)
plot(check$delta ~ check$Region)
Asphalt.index = check$LC=="Asphalt"
Asphalt.index
check[Asphalt.index,]
Forest.index = check$LC == 'Forest'
Forest.index
check[Forest.index,]
mean(check[Asphalt.index,'delta'])
mean(check[Forest.index,'delta'])
tapply(check$delta,check$LC,mean)
cbind(tapply(check$delta,check$LC,mean))
# also try lapply (returns list) and sapply (returns simple string)
#what about variance within the LC type
cbind(tapply(check$delta,check$LC,var))
# or, more intuitively, standard deviation
cbind(sqrt(tapply(check$delta,check$LC,var)))
cbind(tapply(check$delta,check$LC,sd))
# or range
cbind(tapply(check$delta,check$LC,range))
cbind(tapply(check$delta,check$LC,min))
cbind(tapply(check$delta,check$LC,max))
cbind(tapply(check$delta,check$LC,quantile))
tapply(check$delta,check$LC,quantile)
delta.cut = cut(delta,breaks,right=FALSE)
delta.cut
# now we can tabulate the delta values by the interval in which they fall using the table function which counts by levels
delta.freq = table(delta.cut)
cbind(delta.freq)
# it is more efficient to use the hist (histogram) function:
# produces a very useful histogram object and can compute breaks using the rules described in BBR ('Scott','Sturges',others)
delta.hist = hist(delta,breaks='Scott',main='GPS - LiDAR')
# look at delta.hist closely...
summary(delta.hist)
str(delta.hist)
# ...and extract midpoints of the intervals
delta.hist$mids
plot(delta.hist)
plot(delta.hist$mids)
plot(delta.hist$mids, delta.hist$counts)
plot(delta.hist$mids, delta.hist$density)
delta.hist = hist(delta,breaks='Scott',main='GPS - LiDAR')
summary(delta.hist)
str(delta.hist)
summary(delta)
